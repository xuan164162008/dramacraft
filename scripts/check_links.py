#!/usr/bin/env python3
"""
é“¾æ¥æ£€æŸ¥è„šæœ¬

æ£€æŸ¥æ–‡æ¡£ä¸­çš„å†…éƒ¨å’Œå¤–éƒ¨é“¾æ¥æœ‰æ•ˆæ€§
"""

import re
import sys
import asyncio
import aiohttp
from pathlib import Path
from typing import List, Dict, Tuple, Set
from urllib.parse import urljoin, urlparse

class LinkChecker:
    def __init__(self, docs_dir: Path):
        self.docs_dir = docs_dir
        self.base_url = "https://agions.github.io/dramacraft/"
        self.internal_links: Set[str] = set()
        self.external_links: Set[str] = set()
        self.broken_links: List[Dict] = []
        self.valid_pages: Set[str] = set()
        
    def collect_pages(self) -> None:
        """æ”¶é›†æ‰€æœ‰æœ‰æ•ˆé¡µé¢"""
        for md_file in self.docs_dir.rglob("*.md"):
            rel_path = md_file.relative_to(self.docs_dir)
            
            # è½¬æ¢ä¸ºURLè·¯å¾„
            if rel_path.name == "index.md":
                if rel_path.parent == Path("."):
                    url_path = ""
                else:
                    url_path = str(rel_path.parent) + "/"
            else:
                url_path = str(rel_path.with_suffix("")) + "/"
            
            self.valid_pages.add(url_path)
            
            # ä¹Ÿæ·»åŠ å¸¦.htmlçš„ç‰ˆæœ¬
            if url_path.endswith("/"):
                html_path = url_path + "index.html"
            else:
                html_path = url_path + ".html"
            self.valid_pages.add(html_path)
    
    def extract_links_from_file(self, file_path: Path) -> Tuple[List[str], List[str]]:
        """ä»æ–‡ä»¶ä¸­æå–é“¾æ¥"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
            return [], []
        
        # æå–Markdowné“¾æ¥
        md_links = re.findall(r'\[([^\]]*)\]\(([^)]+)\)', content)
        
        # æå–HTMLé“¾æ¥
        html_links = re.findall(r'href=["\']([^"\']+)["\']', content)
        
        internal_links = []
        external_links = []
        
        # å¤„ç†Markdowné“¾æ¥
        for text, url in md_links:
            url = url.split('#')[0]  # ç§»é™¤é”šç‚¹
            if url.startswith('http'):
                external_links.append(url)
            elif url and not url.startswith('mailto:'):
                internal_links.append(url)
        
        # å¤„ç†HTMLé“¾æ¥
        for url in html_links:
            url = url.split('#')[0]  # ç§»é™¤é”šç‚¹
            if url.startswith('http'):
                external_links.append(url)
            elif url and not url.startswith('mailto:') and not url.startswith('javascript:'):
                internal_links.append(url)
        
        return internal_links, external_links
    
    def normalize_internal_link(self, link: str, current_file: Path) -> str:
        """æ ‡å‡†åŒ–å†…éƒ¨é“¾æ¥"""
        if link.startswith('/'):
            # ç»å¯¹è·¯å¾„
            return link[1:]  # ç§»é™¤å¼€å¤´çš„/
        else:
            # ç›¸å¯¹è·¯å¾„
            current_dir = current_file.parent.relative_to(self.docs_dir)
            if current_dir == Path("."):
                return link
            else:
                return str(current_dir / link)
    
    def check_internal_links(self) -> None:
        """æ£€æŸ¥å†…éƒ¨é“¾æ¥"""
        print("ğŸ” æ£€æŸ¥å†…éƒ¨é“¾æ¥...")
        
        for md_file in self.docs_dir.rglob("*.md"):
            internal_links, _ = self.extract_links_from_file(md_file)
            
            for link in internal_links:
                normalized_link = self.normalize_internal_link(link, md_file)
                
                # æ ‡å‡†åŒ–è·¯å¾„
                if normalized_link.endswith('/'):
                    check_paths = [normalized_link, normalized_link + "index.html"]
                elif '.' not in normalized_link.split('/')[-1]:
                    check_paths = [normalized_link + "/", normalized_link + "/index.html"]
                else:
                    check_paths = [normalized_link]
                
                # æ£€æŸ¥æ˜¯å¦å­˜åœ¨
                found = any(path in self.valid_pages for path in check_paths)
                
                if not found:
                    self.broken_links.append({
                        'type': 'internal',
                        'file': str(md_file.relative_to(self.docs_dir)),
                        'link': link,
                        'normalized': normalized_link,
                        'reason': 'Page not found'
                    })
    
    async def check_external_link(self, session: aiohttp.ClientSession, url: str) -> Tuple[str, bool, str]:
        """æ£€æŸ¥å•ä¸ªå¤–éƒ¨é“¾æ¥"""
        try:
            async with session.head(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status < 400:
                    return url, True, f"OK ({response.status})"
                else:
                    return url, False, f"HTTP {response.status}"
        except asyncio.TimeoutError:
            return url, False, "Timeout"
        except Exception as e:
            return url, False, str(e)
    
    async def check_external_links(self) -> None:
        """æ£€æŸ¥å¤–éƒ¨é“¾æ¥"""
        print("ğŸŒ æ£€æŸ¥å¤–éƒ¨é“¾æ¥...")
        
        # æ”¶é›†æ‰€æœ‰å¤–éƒ¨é“¾æ¥
        all_external_links = set()
        for md_file in self.docs_dir.rglob("*.md"):
            _, external_links = self.extract_links_from_file(md_file)
            all_external_links.update(external_links)
        
        if not all_external_links:
            print("âœ… æœªæ‰¾åˆ°å¤–éƒ¨é“¾æ¥")
            return
        
        # å¼‚æ­¥æ£€æŸ¥é“¾æ¥
        async with aiohttp.ClientSession() as session:
            tasks = [self.check_external_link(session, url) for url in all_external_links]
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # å¤„ç†ç»“æœ
        for result in results:
            if isinstance(result, Exception):
                continue
            
            url, is_valid, reason = result
            if not is_valid:
                self.broken_links.append({
                    'type': 'external',
                    'link': url,
                    'reason': reason
                })
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ£€æŸ¥æŠ¥å‘Š"""
        report = "# ğŸ”— é“¾æ¥æ£€æŸ¥æŠ¥å‘Š\n\n"
        
        if not self.broken_links:
            report += "âœ… **æ‰€æœ‰é“¾æ¥éƒ½æœ‰æ•ˆï¼**\n\n"
            report += f"- æ£€æŸ¥äº† {len(self.valid_pages)} ä¸ªé¡µé¢\n"
            report += f"- æ‰€æœ‰å†…éƒ¨é“¾æ¥éƒ½æŒ‡å‘æœ‰æ•ˆé¡µé¢\n"
            report += f"- æ‰€æœ‰å¤–éƒ¨é“¾æ¥éƒ½å¯è®¿é—®\n"
            return report
        
        # åˆ†ç±»ç»Ÿè®¡
        internal_broken = [link for link in self.broken_links if link['type'] == 'internal']
        external_broken = [link for link in self.broken_links if link['type'] == 'external']
        
        report += f"âŒ **å‘ç° {len(self.broken_links)} ä¸ªæ— æ•ˆé“¾æ¥**\n\n"
        
        if internal_broken:
            report += f"## å†…éƒ¨é“¾æ¥é—®é¢˜ ({len(internal_broken)}ä¸ª)\n\n"
            for link in internal_broken:
                report += f"- **{link['file']}**: `{link['link']}` - {link['reason']}\n"
            report += "\n"
        
        if external_broken:
            report += f"## å¤–éƒ¨é“¾æ¥é—®é¢˜ ({len(external_broken)}ä¸ª)\n\n"
            for link in external_broken:
                report += f"- `{link['link']}` - {link['reason']}\n"
            report += "\n"
        
        return report
    
    async def run_check(self) -> bool:
        """è¿è¡Œå®Œæ•´æ£€æŸ¥"""
        print("ğŸš€ å¼€å§‹é“¾æ¥æ£€æŸ¥...")
        
        # æ”¶é›†é¡µé¢
        self.collect_pages()
        print(f"ğŸ“„ å‘ç° {len(self.valid_pages)} ä¸ªæœ‰æ•ˆé¡µé¢")
        
        # æ£€æŸ¥å†…éƒ¨é“¾æ¥
        self.check_internal_links()
        
        # æ£€æŸ¥å¤–éƒ¨é“¾æ¥
        await self.check_external_links()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_report()
        print(report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = self.docs_dir.parent / "link_check_report.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        print(f"ğŸ“‹ æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        
        # è¿”å›æ˜¯å¦æœ‰é—®é¢˜
        return len(self.broken_links) == 0

async def main():
    """ä¸»å‡½æ•°"""
    docs_dir = Path(__file__).parent.parent / "docs"
    
    if not docs_dir.exists():
        print("âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨")
        sys.exit(1)
    
    checker = LinkChecker(docs_dir)
    success = await checker.run_check()
    
    if success:
        print("âœ… é“¾æ¥æ£€æŸ¥é€šè¿‡")
        sys.exit(0)
    else:
        print("âŒ å‘ç°æ— æ•ˆé“¾æ¥")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
