# ğŸ¤– å¤§æ¨¡å‹é›†æˆæ¨¡å—

å¤§æ¨¡å‹é›†æˆæ¨¡å—æä¾›å¯¹å›½äº§ä¸­æ–‡å¤§æ¨¡å‹çš„ç»Ÿä¸€æ¥å£ï¼Œæ”¯æŒç™¾åº¦åƒå¸†ã€é˜¿é‡Œé€šä¹‰ã€è…¾è®¯æ··å…ƒç­‰ä¸»æµå¹³å°ã€‚

## ğŸ¯ æ¨¡å—æ¦‚è¿°

### æ”¯æŒçš„å¤§æ¨¡å‹å¹³å°

- **ğŸŒŸ ç™¾åº¦åƒå¸†**: å®Œæ•´çš„ API é›†æˆå’Œä¼˜åŒ–
- **âš¡ é˜¿é‡Œé€šä¹‰**: åŸç”Ÿ SDK æ”¯æŒ
- **ğŸš€ è…¾è®¯æ··å…ƒ**: ä¸“ä¸šçº§æ¥å£é€‚é…
- **ğŸ¯ æ™ºèƒ½åˆ‡æ¢**: æ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜æ¨¡å‹

### æ ¸å¿ƒç‰¹æ€§

- **ç»Ÿä¸€æ¥å£**: æ‰€æœ‰å¹³å°ä½¿ç”¨ç›¸åŒçš„APIæ¥å£
- **å¼‚æ­¥æ”¯æŒ**: åŸºäº asyncio çš„é«˜æ€§èƒ½å¼‚æ­¥è°ƒç”¨
- **é”™è¯¯æ¢å¤**: å®Œå–„çš„é‡è¯•æœºåˆ¶å’Œé”™è¯¯å¤„ç†
- **æ€§èƒ½ç›‘æ§**: å®æ—¶ç›‘æ§APIè°ƒç”¨æ€§èƒ½å’ŒæˆåŠŸç‡
- **æ™ºèƒ½è·¯ç”±**: æ ¹æ®ä»»åŠ¡ç±»å‹å’Œè´Ÿè½½è‡ªåŠ¨é€‰æ‹©æ¨¡å‹

### ä¸»è¦ç»„ä»¶

- `BaseLLMClient`: åŸºç¡€LLMå®¢æˆ·ç«¯æŠ½è±¡ç±»
- `BaiduLLMClient`: ç™¾åº¦åƒå¸†å®¢æˆ·ç«¯
- `AlibabaLLMClient`: é˜¿é‡Œé€šä¹‰å®¢æˆ·ç«¯
- `TencentLLMClient`: è…¾è®¯æ··å…ƒå®¢æˆ·ç«¯
- `LLMFactory`: LLMå®¢æˆ·ç«¯å·¥å‚

## ğŸ”§ API å‚è€ƒ

### BaseLLMClient

```python
from dramacraft.llm.base import BaseLLMClient, GenerationResult, GenerationParams

class BaseLLMClient:
    """LLMå®¢æˆ·ç«¯åŸºç±»ã€‚"""
    
    async def generate(
        self, 
        prompt: str, 
        params: Optional[GenerationParams] = None
    ) -> GenerationResult:
        """
        ç”Ÿæˆæ–‡æœ¬å†…å®¹ã€‚
        
        Args:
            prompt: è¾“å…¥æç¤ºè¯
            params: ç”Ÿæˆå‚æ•°
            
        Returns:
            ç”Ÿæˆç»“æœ
        """
```

### GenerationParams

```python
@dataclass
class GenerationParams:
    """ç”Ÿæˆå‚æ•°ã€‚"""
    
    temperature: float = 0.7
    """æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ã€‚"""
    
    max_tokens: int = 2000
    """æœ€å¤§ç”Ÿæˆä»¤ç‰Œæ•°ã€‚"""
    
    top_p: float = 0.9
    """Top-pé‡‡æ ·å‚æ•°ã€‚"""
    
    frequency_penalty: float = 0.0
    """é¢‘ç‡æƒ©ç½šã€‚"""
    
    presence_penalty: float = 0.0
    """å­˜åœ¨æƒ©ç½šã€‚"""
    
    stop_sequences: Optional[List[str]] = None
    """åœæ­¢åºåˆ—ã€‚"""
```

### GenerationResult

```python
@dataclass
class GenerationResult:
    """ç”Ÿæˆç»“æœã€‚"""
    
    text: str
    """ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ã€‚"""
    
    usage: Dict[str, int]
    """ä»¤ç‰Œä½¿ç”¨ç»Ÿè®¡ã€‚"""
    
    model: Optional[str] = None
    """ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚"""
    
    finish_reason: Optional[str] = None
    """å®ŒæˆåŸå› ã€‚"""
    
    response_time: Optional[float] = None
    """å“åº”æ—¶é—´(ç§’)ã€‚"""
```

### LLMå·¥å‚

```python
from dramacraft.llm.factory import create_llm_client
from dramacraft.config import LLMConfig

def create_llm_client(config: LLMConfig) -> BaseLLMClient:
    """
    åˆ›å»ºLLMå®¢æˆ·ç«¯ã€‚
    
    Args:
        config: LLMé…ç½®
        
    Returns:
        LLMå®¢æˆ·ç«¯å®ä¾‹
    """
```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
import asyncio
from dramacraft.llm.factory import create_llm_client
from dramacraft.config import DramaCraftConfig

async def basic_llm_example():
    """åŸºç¡€LLMä½¿ç”¨ç¤ºä¾‹ã€‚"""
    
    # åŠ è½½é…ç½®
    config = DramaCraftConfig()
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = create_llm_client(config.llm)
    
    # ç”Ÿæˆæ–‡æœ¬
    prompt = "è¯·ä¸ºè¿™ä¸ªçŸ­å‰§åœºæ™¯ç”Ÿæˆæç¬‘çš„è§£è¯´æ–‡æ¡ˆï¼šä¸¤ä¸ªäººåœ¨å’–å•¡å…é‡Œå°´å°¬åœ°ç›¸è§†è€Œåã€‚"
    result = await llm_client.generate(prompt)
    
    print(f"ç”Ÿæˆå†…å®¹: {result.text}")
    print(f"ä»¤ç‰Œä½¿ç”¨: {result.usage}")
    print(f"å“åº”æ—¶é—´: {result.response_time:.2f}ç§’")

asyncio.run(basic_llm_example())
```

### è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°

```python
from dramacraft.llm.base import GenerationParams

async def custom_params_example():
    """è‡ªå®šä¹‰å‚æ•°ç¤ºä¾‹ã€‚"""
    
    config = DramaCraftConfig()
    llm_client = create_llm_client(config.llm)
    
    # è‡ªå®šä¹‰ç”Ÿæˆå‚æ•°
    params = GenerationParams(
        temperature=0.8,        # æé«˜åˆ›é€ æ€§
        max_tokens=1000,        # é™åˆ¶é•¿åº¦
        top_p=0.95,            # è°ƒæ•´é‡‡æ ·
        stop_sequences=["ã€‚", "ï¼", "ï¼Ÿ"]  # åœæ­¢æ¡ä»¶
    )
    
    prompt = "åˆ›ä½œä¸€ä¸ªæç¬‘çš„çŸ­å‰§å¼€åœºç™½"
    result = await llm_client.generate(prompt, params)
    
    print(f"åˆ›ä½œå†…å®¹: {result.text}")

asyncio.run(custom_params_example())
```

### æ‰¹é‡ç”Ÿæˆ

```python
async def batch_generation_example():
    """æ‰¹é‡ç”Ÿæˆç¤ºä¾‹ã€‚"""
    
    config = DramaCraftConfig()
    llm_client = create_llm_client(config.llm)
    
    # æ‰¹é‡æç¤ºè¯
    prompts = [
        "ä¸ºæç¬‘çŸ­å‰§ç”Ÿæˆå¼€åœºç™½",
        "ä¸ºæ‚¬ç–‘çŸ­å‰§ç”Ÿæˆå¼€åœºç™½", 
        "ä¸ºçˆ±æƒ…çŸ­å‰§ç”Ÿæˆå¼€åœºç™½"
    ]
    
    # å¹¶å‘ç”Ÿæˆ
    tasks = [llm_client.generate(prompt) for prompt in prompts]
    results = await asyncio.gather(*tasks)
    
    for i, result in enumerate(results):
        print(f"æç¤ºè¯ {i+1}: {result.text[:50]}...")

asyncio.run(batch_generation_example())
```

### é”™è¯¯å¤„ç†

```python
from dramacraft.llm.exceptions import LLMError, RateLimitError, AuthenticationError

async def error_handling_example():
    """é”™è¯¯å¤„ç†ç¤ºä¾‹ã€‚"""
    
    config = DramaCraftConfig()
    llm_client = create_llm_client(config.llm)
    
    try:
        result = await llm_client.generate("ç”Ÿæˆå†…å®¹")
        print(f"æˆåŠŸ: {result.text}")
        
    except AuthenticationError:
        print("è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥APIå¯†é’¥")
        
    except RateLimitError:
        print("è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•")
        
    except LLMError as e:
        print(f"LLMè°ƒç”¨å¤±è´¥: {e}")
        
    except Exception as e:
        print(f"æœªçŸ¥é”™è¯¯: {e}")

asyncio.run(error_handling_example())
```

## âš™ï¸ é…ç½®è¯´æ˜

### ç™¾åº¦åƒå¸†é…ç½®

```python
llm_config = {
    "provider": "baidu",
    "api_key": "your_api_key",
    "secret_key": "your_secret_key",
    "model": "ernie-bot-turbo",
    "endpoint": "https://aip.baidubce.com",
    "timeout": 30.0,
    "max_retries": 3,
    "retry_delay": 1.0
}
```

### é˜¿é‡Œé€šä¹‰é…ç½®

```python
llm_config = {
    "provider": "alibaba", 
    "api_key": "your_api_key",
    "model": "qwen-turbo",
    "region": "cn-beijing",
    "timeout": 30.0,
    "max_retries": 3
}
```

### è…¾è®¯æ··å…ƒé…ç½®

```python
llm_config = {
    "provider": "tencent",
    "secret_id": "your_secret_id", 
    "secret_key": "your_secret_key",
    "model": "hunyuan-lite",
    "region": "ap-beijing",
    "timeout": 30.0,
    "max_retries": 3
}
```

### é€šç”¨é…ç½®é€‰é¡¹

```python
llm_config = {
    # åŸºç¡€é…ç½®
    "provider": "baidu",           # æä¾›å•†: baidu/alibaba/tencent
    "model": "ernie-bot-turbo",    # æ¨¡å‹åç§°
    
    # è®¤è¯é…ç½®
    "api_key": "your_api_key",     # APIå¯†é’¥
    "secret_key": "your_secret",   # å¯†é’¥(ç™¾åº¦éœ€è¦)
    
    # ç”Ÿæˆå‚æ•°
    "temperature": 0.7,            # é»˜è®¤æ¸©åº¦
    "max_tokens": 2000,            # é»˜è®¤æœ€å¤§ä»¤ç‰Œ
    "top_p": 0.9,                  # é»˜è®¤top_p
    
    # æ€§èƒ½é…ç½®
    "timeout": 30.0,               # è¯·æ±‚è¶…æ—¶(ç§’)
    "max_retries": 3,              # æœ€å¤§é‡è¯•æ¬¡æ•°
    "retry_delay": 1.0,            # é‡è¯•å»¶è¿Ÿ(ç§’)
    "rate_limit": 10,              # æ¯ç§’è¯·æ±‚é™åˆ¶
    
    # ç¼“å­˜é…ç½®
    "cache_enabled": True,         # æ˜¯å¦å¯ç”¨ç¼“å­˜
    "cache_ttl": 3600,            # ç¼“å­˜è¿‡æœŸæ—¶é—´(ç§’)
    
    # ç›‘æ§é…ç½®
    "monitoring_enabled": True,    # æ˜¯å¦å¯ç”¨ç›‘æ§
    "log_requests": False         # æ˜¯å¦è®°å½•è¯·æ±‚æ—¥å¿—
}
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

**Q: APIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›è®¤è¯é”™è¯¯ï¼Ÿ**
A: æ£€æŸ¥ä»¥ä¸‹é…ç½®ï¼š
- ç¡®è®¤APIå¯†é’¥æ­£ç¡®ä¸”æœ‰æ•ˆ
- æ£€æŸ¥å¯†é’¥æ˜¯å¦æœ‰ç›¸åº”æƒé™
- éªŒè¯è´¦æˆ·ä½™é¢æ˜¯å¦å……è¶³
- ç¡®è®¤ç½‘ç»œè¿æ¥æ­£å¸¸

**Q: è¯·æ±‚è¶…æ—¶æ€ä¹ˆåŠï¼Ÿ**
A: å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š
- å¢åŠ  `timeout` è®¾ç½®
- æ£€æŸ¥ç½‘ç»œè¿æ¥ç¨³å®šæ€§
- å‡å°‘ `max_tokens` é™ä½å¤„ç†æ—¶é—´
- å¯ç”¨é‡è¯•æœºåˆ¶

**Q: ç”Ÿæˆè´¨é‡ä¸ç†æƒ³ï¼Ÿ**
A: è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š
- ä¼˜åŒ–æç¤ºè¯çš„æè¿°å’Œæ ¼å¼
- è°ƒæ•´ `temperature` æ§åˆ¶åˆ›é€ æ€§
- ä¿®æ”¹ `top_p` å½±å“å¤šæ ·æ€§
- å°è¯•ä¸åŒçš„æ¨¡å‹

**Q: è¯·æ±‚é¢‘ç‡é™åˆ¶ï¼Ÿ**
A: å¤„ç†æ–¹æ³•ï¼š
- è®¾ç½®åˆç†çš„ `rate_limit`
- ä½¿ç”¨æŒ‡æ•°é€€é¿é‡è¯•
- è€ƒè™‘å‡çº§APIå¥—é¤
- å®ç°è¯·æ±‚é˜Ÿåˆ—ç®¡ç†

### æ€§èƒ½ä¼˜åŒ–

1. **ç¼“å­˜ç­–ç•¥**: å¯ç”¨ç¼“å­˜é¿å…é‡å¤è¯·æ±‚
2. **å¹¶å‘æ§åˆ¶**: åˆç†è®¾ç½®å¹¶å‘æ•°é‡
3. **å‚æ•°è°ƒä¼˜**: æ ¹æ®ä»»åŠ¡è°ƒæ•´ç”Ÿæˆå‚æ•°
4. **é”™è¯¯æ¢å¤**: å®ç°æ™ºèƒ½é‡è¯•æœºåˆ¶
5. **ç›‘æ§å‘Šè­¦**: è®¾ç½®æ€§èƒ½ç›‘æ§å’Œå‘Šè­¦

### è°ƒè¯•æŠ€å·§

```python
# å¯ç”¨è¯¦ç»†æ—¥å¿—
import logging
logging.getLogger("dramacraft.llm").setLevel(logging.DEBUG)

# ç›‘æ§APIè°ƒç”¨
from dramacraft.monitoring.performance import get_performance_monitor
monitor = get_performance_monitor()
metrics = monitor.get_llm_metrics()

# æµ‹è¯•è¿æ¥
async def test_connection():
    try:
        result = await llm_client.generate("æµ‹è¯•")
        print("è¿æ¥æ­£å¸¸")
    except Exception as e:
        print(f"è¿æ¥å¤±è´¥: {e}")
```

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [é…ç½®ç®¡ç†](../../config.py)
- [æ€§èƒ½ç›‘æ§æ¨¡å—](../monitoring/docs/README.md)
- [é”™è¯¯å¤„ç†æŒ‡å—](../../../docs/troubleshooting.md)
- [APIä½¿ç”¨ç¤ºä¾‹](../../../examples/llm_examples.py)
